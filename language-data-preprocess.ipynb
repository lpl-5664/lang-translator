{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb4d5b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:22.663621Z",
     "iopub.status.busy": "2024-04-08T00:22:22.663123Z",
     "iopub.status.idle": "2024-04-08T00:22:38.856370Z",
     "shell.execute_reply": "2024-04-08T00:22:38.855037Z"
    },
    "papermill": {
     "duration": 16.203828,
     "end_time": "2024-04-08T00:22:38.859295",
     "exception": false,
     "start_time": "2024-04-08T00:22:22.655467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 00:22:24.668031: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 00:22:24.668192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 00:22:24.805858: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dddd3",
   "metadata": {
    "papermill": {
     "duration": 0.00551,
     "end_time": "2024-04-08T00:22:38.870809",
     "exception": false,
     "start_time": "2024-04-08T00:22:38.865299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd418cf",
   "metadata": {
    "papermill": {
     "duration": 0.005436,
     "end_time": "2024-04-08T00:22:38.882625",
     "exception": false,
     "start_time": "2024-04-08T00:22:38.877189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895dbd39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:38.896349Z",
     "iopub.status.busy": "2024-04-08T00:22:38.895532Z",
     "iopub.status.idle": "2024-04-08T00:22:38.903788Z",
     "shell.execute_reply": "2024-04-08T00:22:38.902198Z"
    },
    "papermill": {
     "duration": 0.018037,
     "end_time": "2024-04-08T00:22:38.906326",
     "exception": false,
     "start_time": "2024-04-08T00:22:38.888289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t')[:2] for line in lines]\n",
    "\n",
    "    src = np.array([src for tgt, src in pairs])\n",
    "    tgt = np.array([tgt for tgt, src in pairs])\n",
    "\n",
    "    return tgt, src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf28ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:38.919756Z",
     "iopub.status.busy": "2024-04-08T00:22:38.919277Z",
     "iopub.status.idle": "2024-04-08T00:22:39.328936Z",
     "shell.execute_reply": "2024-04-08T00:22:39.327291Z"
    },
    "papermill": {
     "duration": 0.420053,
     "end_time": "2024-04-08T00:22:39.332069",
     "exception": false,
     "start_time": "2024-04-08T00:22:38.912016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#path_txt = '/kaggle/input/translator-data/kor.txt'\n",
    "#path_txt = '/kaggle/input/translator-data/spa.txt'\n",
    "path_txt = '/kaggle/input/translator-data/vie.txt'\n",
    "path = pathlib.Path(path_txt)\n",
    "tgt_raw, src_raw = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392de59",
   "metadata": {
    "papermill": {
     "duration": 0.005572,
     "end_time": "2024-04-08T00:22:39.344075",
     "exception": false,
     "start_time": "2024-04-08T00:22:39.338503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Create a tf.data Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255fd4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:39.358449Z",
     "iopub.status.busy": "2024-04-08T00:22:39.357938Z",
     "iopub.status.idle": "2024-04-08T00:22:39.448187Z",
     "shell.execute_reply": "2024-04-08T00:22:39.446793Z"
    },
    "papermill": {
     "duration": 0.101392,
     "end_time": "2024-04-08T00:22:39.451291",
     "exception": false,
     "start_time": "2024-04-08T00:22:39.349899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(src_raw)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(tgt_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((src_raw[is_train], tgt_raw[is_train])))\n",
    "    #.shuffle(BUFFER_SIZE)\n",
    "    #.batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((src_raw[~is_train], tgt_raw[~is_train])))\n",
    "    #.shuffle(BUFFER_SIZE)\n",
    "    #.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa1be6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:39.465113Z",
     "iopub.status.busy": "2024-04-08T00:22:39.464498Z",
     "iopub.status.idle": "2024-04-08T00:22:39.517408Z",
     "shell.execute_reply": "2024-04-08T00:22:39.516058Z"
    },
    "papermill": {
     "duration": 0.063052,
     "end_time": "2024-04-08T00:22:39.520132",
     "exception": false,
     "start_time": "2024-04-08T00:22:39.457080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  Chạy đi!\n",
      "Target:  Run!\n"
     ]
    }
   ],
   "source": [
    "for src, tgt in train_raw.take(1):\n",
    "    print(\"Source: \", src.numpy().decode('utf-8'))\n",
    "    print(\"Target: \", tgt.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90375bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:39.535255Z",
     "iopub.status.busy": "2024-04-08T00:22:39.534782Z",
     "iopub.status.idle": "2024-04-08T00:22:39.587937Z",
     "shell.execute_reply": "2024-04-08T00:22:39.586631Z"
    },
    "papermill": {
     "duration": 0.064207,
     "end_time": "2024-04-08T00:22:39.590872",
     "exception": false,
     "start_time": "2024-04-08T00:22:39.526665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tgt = train_raw.map(lambda src, tgt: tgt)\n",
    "train_src = train_raw.map(lambda src, tgt: src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2d427",
   "metadata": {
    "papermill": {
     "duration": 0.005475,
     "end_time": "2024-04-08T00:22:39.602565",
     "exception": false,
     "start_time": "2024-04-08T00:22:39.597090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Generate the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71ea1c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:39.616117Z",
     "iopub.status.busy": "2024-04-08T00:22:39.615657Z",
     "iopub.status.idle": "2024-04-08T00:22:39.627228Z",
     "shell.execute_reply": "2024-04-08T00:22:39.626106Z"
    },
    "papermill": {
     "duration": 0.021791,
     "end_time": "2024-04-08T00:22:39.629990",
     "exception": false,
     "start_time": "2024-04-08T00:22:39.608199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76bb076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:39.645209Z",
     "iopub.status.busy": "2024-04-08T00:22:39.644742Z",
     "iopub.status.idle": "2024-04-08T00:22:39.650605Z",
     "shell.execute_reply": "2024-04-08T00:22:39.649280Z"
    },
    "papermill": {
     "duration": 0.015571,
     "end_time": "2024-04-08T00:22:39.652852",
     "exception": false,
     "start_time": "2024-04-08T00:22:39.637281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1875d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:39.666658Z",
     "iopub.status.busy": "2024-04-08T00:22:39.665851Z",
     "iopub.status.idle": "2024-04-08T00:22:41.495838Z",
     "shell.execute_reply": "2024-04-08T00:22:41.494172Z"
    },
    "papermill": {
     "duration": 1.840022,
     "end_time": "2024-04-08T00:22:41.498688",
     "exception": false,
     "start_time": "2024-04-08T00:22:39.658666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 64.1 ms, total: 2.01 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "src_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_src.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b328292b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:41.512555Z",
     "iopub.status.busy": "2024-04-08T00:22:41.512078Z",
     "iopub.status.idle": "2024-04-08T00:22:49.688986Z",
     "shell.execute_reply": "2024-04-08T00:22:49.687423Z"
    },
    "papermill": {
     "duration": 8.187014,
     "end_time": "2024-04-08T00:22:49.691670",
     "exception": false,
     "start_time": "2024-04-08T00:22:41.504656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.27 s, sys: 56.7 ms, total: 8.32 s\n",
      "Wall time: 8.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tgt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_tgt.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bcc5e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:49.705887Z",
     "iopub.status.busy": "2024-04-08T00:22:49.705425Z",
     "iopub.status.idle": "2024-04-08T00:22:49.711489Z",
     "shell.execute_reply": "2024-04-08T00:22:49.710368Z"
    },
    "papermill": {
     "duration": 0.015919,
     "end_time": "2024-04-08T00:22:49.713845",
     "exception": false,
     "start_time": "2024-04-08T00:22:49.697926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b130917d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:49.727939Z",
     "iopub.status.busy": "2024-04-08T00:22:49.727294Z",
     "iopub.status.idle": "2024-04-08T00:22:49.734767Z",
     "shell.execute_reply": "2024-04-08T00:22:49.733376Z"
    },
    "papermill": {
     "duration": 0.017685,
     "end_time": "2024-04-08T00:22:49.737476",
     "exception": false,
     "start_time": "2024-04-08T00:22:49.719791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_vocab_file('vie_vocab.txt', src_vocab)\n",
    "write_vocab_file('en_vocab.txt', tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470230eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:49.751322Z",
     "iopub.status.busy": "2024-04-08T00:22:49.750834Z",
     "iopub.status.idle": "2024-04-08T00:22:49.770293Z",
     "shell.execute_reply": "2024-04-08T00:22:49.768967Z"
    },
    "papermill": {
     "duration": 0.029821,
     "end_time": "2024-04-08T00:22:49.773307",
     "exception": false,
     "start_time": "2024-04-08T00:22:49.743486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f613b28f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:49.788350Z",
     "iopub.status.busy": "2024-04-08T00:22:49.787238Z",
     "iopub.status.idle": "2024-04-08T00:22:49.795020Z",
     "shell.execute_reply": "2024-04-08T00:22:49.793807Z"
    },
    "papermill": {
     "duration": 0.017613,
     "end_time": "2024-04-08T00:22:49.797385",
     "exception": false,
     "start_time": "2024-04-08T00:22:49.779772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4543bd06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:49.811056Z",
     "iopub.status.busy": "2024-04-08T00:22:49.810606Z",
     "iopub.status.idle": "2024-04-08T00:22:49.825415Z",
     "shell.execute_reply": "2024-04-08T00:22:49.824168Z"
    },
    "papermill": {
     "duration": 0.024478,
     "end_time": "2024-04-08T00:22:49.827686",
     "exception": false,
     "start_time": "2024-04-08T00:22:49.803208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        self.tokenizer = tf_text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:   \n",
    "\n",
    "        # Include a tokenize signature for a batch of strings. \n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "              tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "              tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2,-1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20006237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:49.842258Z",
     "iopub.status.busy": "2024-04-08T00:22:49.841088Z",
     "iopub.status.idle": "2024-04-08T00:22:53.347930Z",
     "shell.execute_reply": "2024-04-08T00:22:53.344383Z"
    },
    "papermill": {
     "duration": 3.518247,
     "end_time": "2024-04-08T00:22:53.351966",
     "exception": false,
     "start_time": "2024-04-08T00:22:49.833719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.vie = CustomTokenizer(reserved_tokens, '/kaggle/working/vie_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, '/kaggle/working/en_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5501d5e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:22:53.368183Z",
     "iopub.status.busy": "2024-04-08T00:22:53.367675Z",
     "iopub.status.idle": "2024-04-08T00:22:56.628740Z",
     "shell.execute_reply": "2024-04-08T00:22:56.627431Z"
    },
    "papermill": {
     "duration": 3.271887,
     "end_time": "2024-04-08T00:22:56.631739",
     "exception": false,
     "start_time": "2024-04-08T00:22:53.359852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'vie_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4748462,
     "sourceId": 8051857,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40.174971,
   "end_time": "2024-04-08T00:22:59.819669",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T00:22:19.644698",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
