{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472b40fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:34.578303Z",
     "iopub.status.busy": "2024-04-07T07:34:34.577634Z",
     "iopub.status.idle": "2024-04-07T07:34:53.080916Z",
     "shell.execute_reply": "2024-04-07T07:34:53.079577Z"
    },
    "papermill": {
     "duration": 18.517397,
     "end_time": "2024-04-07T07:34:53.084151",
     "exception": false,
     "start_time": "2024-04-07T07:34:34.566754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 07:34:37.115822: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 07:34:37.115955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 07:34:37.313086: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c7c8b",
   "metadata": {
    "papermill": {
     "duration": 0.008256,
     "end_time": "2024-04-07T07:34:53.100774",
     "exception": false,
     "start_time": "2024-04-07T07:34:53.092518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152b22b",
   "metadata": {
    "papermill": {
     "duration": 0.00763,
     "end_time": "2024-04-07T07:34:53.116299",
     "exception": false,
     "start_time": "2024-04-07T07:34:53.108669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26833955",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:53.135909Z",
     "iopub.status.busy": "2024-04-07T07:34:53.134920Z",
     "iopub.status.idle": "2024-04-07T07:34:54.031762Z",
     "shell.execute_reply": "2024-04-07T07:34:54.030792Z"
    },
    "papermill": {
     "duration": 0.909756,
     "end_time": "2024-04-07T07:34:54.034418",
     "exception": false,
     "start_time": "2024-04-07T07:34:53.124662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "\u001b[1m2638744/2638744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
    "\n",
    "#path_to_zip = get_file(\n",
    "#    'kor-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/kor-eng.zip',\n",
    "#    extract=True))\n",
    "#path_to_file = pathlib.Path(path_to_zip).parent/'kor-eng/kor.txt'\n",
    "\n",
    "#path_to_zip = get_file(\n",
    "#    'kor-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/kor-eng.zip',\n",
    "#    extract=True))\n",
    "#path_to_file = pathlib.Path(path_to_zip).parent/'kor-eng/kor.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4ea81",
   "metadata": {
    "papermill": {
     "duration": 0.009924,
     "end_time": "2024-04-07T07:34:54.055454",
     "exception": false,
     "start_time": "2024-04-07T07:34:54.045530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7f4f5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:54.077433Z",
     "iopub.status.busy": "2024-04-07T07:34:54.076716Z",
     "iopub.status.idle": "2024-04-07T07:34:54.084340Z",
     "shell.execute_reply": "2024-04-07T07:34:54.083085Z"
    },
    "papermill": {
     "duration": 0.022359,
     "end_time": "2024-04-07T07:34:54.087259",
     "exception": false,
     "start_time": "2024-04-07T07:34:54.064900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    src = np.array([src for tgt, src in pairs])\n",
    "    tgt = np.array([tgt for tgt, src in pairs])\n",
    "\n",
    "    return tgt, src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e9ed3aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:54.109398Z",
     "iopub.status.busy": "2024-04-07T07:34:54.108594Z",
     "iopub.status.idle": "2024-04-07T07:34:54.900679Z",
     "shell.execute_reply": "2024-04-07T07:34:54.898943Z"
    },
    "papermill": {
     "duration": 0.806962,
     "end_time": "2024-04-07T07:34:54.903732",
     "exception": false,
     "start_time": "2024-04-07T07:34:54.096770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt_raw, src_raw = load_data(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b9213",
   "metadata": {
    "papermill": {
     "duration": 0.009027,
     "end_time": "2024-04-07T07:34:54.922399",
     "exception": false,
     "start_time": "2024-04-07T07:34:54.913372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Create a tf.data Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719c2786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:54.944962Z",
     "iopub.status.busy": "2024-04-07T07:34:54.944523Z",
     "iopub.status.idle": "2024-04-07T07:34:55.637070Z",
     "shell.execute_reply": "2024-04-07T07:34:55.635619Z"
    },
    "papermill": {
     "duration": 0.706911,
     "end_time": "2024-04-07T07:34:55.640107",
     "exception": false,
     "start_time": "2024-04-07T07:34:54.933196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(src_raw)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(tgt_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((src_raw[is_train], tgt_raw[is_train])))\n",
    "    #.shuffle(BUFFER_SIZE)\n",
    "    #.batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((src_raw[~is_train], tgt_raw[~is_train])))\n",
    "    #.shuffle(BUFFER_SIZE)\n",
    "    #.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4095ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:55.660821Z",
     "iopub.status.busy": "2024-04-07T07:34:55.660373Z",
     "iopub.status.idle": "2024-04-07T07:34:55.711233Z",
     "shell.execute_reply": "2024-04-07T07:34:55.709787Z"
    },
    "papermill": {
     "duration": 0.064528,
     "end_time": "2024-04-07T07:34:55.713922",
     "exception": false,
     "start_time": "2024-04-07T07:34:55.649394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish:  Ve.\n",
      "English:  Go.\n"
     ]
    }
   ],
   "source": [
    "for spa, en in train_raw.take(1):\n",
    "    print(\"Spanish: \", spa.numpy().decode('utf-8'))\n",
    "    print(\"English: \", en.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643d9371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:55.736433Z",
     "iopub.status.busy": "2024-04-07T07:34:55.735368Z",
     "iopub.status.idle": "2024-04-07T07:34:55.790842Z",
     "shell.execute_reply": "2024-04-07T07:34:55.789491Z"
    },
    "papermill": {
     "duration": 0.069622,
     "end_time": "2024-04-07T07:34:55.793596",
     "exception": false,
     "start_time": "2024-04-07T07:34:55.723974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_en = train_raw.map(lambda spa, en: en)\n",
    "train_spa = train_raw.map(lambda spa, en: spa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c259249",
   "metadata": {
    "papermill": {
     "duration": 0.008872,
     "end_time": "2024-04-07T07:34:55.812798",
     "exception": false,
     "start_time": "2024-04-07T07:34:55.803926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Generate the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c14c307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:55.833358Z",
     "iopub.status.busy": "2024-04-07T07:34:55.832933Z",
     "iopub.status.idle": "2024-04-07T07:34:55.846133Z",
     "shell.execute_reply": "2024-04-07T07:34:55.844319Z"
    },
    "papermill": {
     "duration": 0.027027,
     "end_time": "2024-04-07T07:34:55.848942",
     "exception": false,
     "start_time": "2024-04-07T07:34:55.821915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82d3de98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:55.869912Z",
     "iopub.status.busy": "2024-04-07T07:34:55.869508Z",
     "iopub.status.idle": "2024-04-07T07:34:55.875854Z",
     "shell.execute_reply": "2024-04-07T07:34:55.874574Z"
    },
    "papermill": {
     "duration": 0.019553,
     "end_time": "2024-04-07T07:34:55.878228",
     "exception": false,
     "start_time": "2024-04-07T07:34:55.858675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3332a9ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:34:55.899424Z",
     "iopub.status.busy": "2024-04-07T07:34:55.898975Z",
     "iopub.status.idle": "2024-04-07T07:36:53.900759Z",
     "shell.execute_reply": "2024-04-07T07:36:53.899199Z"
    },
    "papermill": {
     "duration": 118.024562,
     "end_time": "2024-04-07T07:36:53.912102",
     "exception": false,
     "start_time": "2024-04-07T07:34:55.887540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 58s, sys: 628 ms, total: 1min 59s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spa_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_spa.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d60e9fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:36:53.934373Z",
     "iopub.status.busy": "2024-04-07T07:36:53.933931Z",
     "iopub.status.idle": "2024-04-07T07:37:49.604077Z",
     "shell.execute_reply": "2024-04-07T07:37:49.602833Z"
    },
    "papermill": {
     "duration": 55.693135,
     "end_time": "2024-04-07T07:37:49.615734",
     "exception": false,
     "start_time": "2024-04-07T07:36:53.922599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.5 s, sys: 528 ms, total: 57.1 s\n",
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_en.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c380f9f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:37:49.637008Z",
     "iopub.status.busy": "2024-04-07T07:37:49.636551Z",
     "iopub.status.idle": "2024-04-07T07:37:49.642799Z",
     "shell.execute_reply": "2024-04-07T07:37:49.641325Z"
    },
    "papermill": {
     "duration": 0.02038,
     "end_time": "2024-04-07T07:37:49.645623",
     "exception": false,
     "start_time": "2024-04-07T07:37:49.625243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c366e5ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:37:49.666915Z",
     "iopub.status.busy": "2024-04-07T07:37:49.666512Z",
     "iopub.status.idle": "2024-04-07T07:37:49.680241Z",
     "shell.execute_reply": "2024-04-07T07:37:49.679230Z"
    },
    "papermill": {
     "duration": 0.027899,
     "end_time": "2024-04-07T07:37:49.683003",
     "exception": false,
     "start_time": "2024-04-07T07:37:49.655104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_vocab_file('spa_vocab.txt', spa_vocab)\n",
    "write_vocab_file('en_vocab.txt', en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd37ea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:37:49.704272Z",
     "iopub.status.busy": "2024-04-07T07:37:49.703420Z",
     "iopub.status.idle": "2024-04-07T07:37:49.723780Z",
     "shell.execute_reply": "2024-04-07T07:37:49.722677Z"
    },
    "papermill": {
     "duration": 0.03405,
     "end_time": "2024-04-07T07:37:49.726587",
     "exception": false,
     "start_time": "2024-04-07T07:37:49.692537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdc35375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:37:49.747052Z",
     "iopub.status.busy": "2024-04-07T07:37:49.746652Z",
     "iopub.status.idle": "2024-04-07T07:37:49.753990Z",
     "shell.execute_reply": "2024-04-07T07:37:49.752721Z"
    },
    "papermill": {
     "duration": 0.020569,
     "end_time": "2024-04-07T07:37:49.756541",
     "exception": false,
     "start_time": "2024-04-07T07:37:49.735972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf4a614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:37:49.777827Z",
     "iopub.status.busy": "2024-04-07T07:37:49.777363Z",
     "iopub.status.idle": "2024-04-07T07:37:49.795931Z",
     "shell.execute_reply": "2024-04-07T07:37:49.794074Z"
    },
    "papermill": {
     "duration": 0.032987,
     "end_time": "2024-04-07T07:37:49.799144",
     "exception": false,
     "start_time": "2024-04-07T07:37:49.766157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        self.tokenizer = tf_text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:   \n",
    "\n",
    "        # Include a tokenize signature for a batch of strings. \n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "              tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "              tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2,-1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e10cd15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:37:49.821846Z",
     "iopub.status.busy": "2024-04-07T07:37:49.820242Z",
     "iopub.status.idle": "2024-04-07T07:37:53.693907Z",
     "shell.execute_reply": "2024-04-07T07:37:53.692708Z"
    },
    "papermill": {
     "duration": 3.887592,
     "end_time": "2024-04-07T07:37:53.696723",
     "exception": false,
     "start_time": "2024-04-07T07:37:49.809131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.spa = CustomTokenizer(reserved_tokens, '/kaggle/working/spa_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, '/kaggle/working/en_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d636eecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T07:37:53.717799Z",
     "iopub.status.busy": "2024-04-07T07:37:53.717213Z",
     "iopub.status.idle": "2024-04-07T07:37:56.992421Z",
     "shell.execute_reply": "2024-04-07T07:37:56.991159Z"
    },
    "papermill": {
     "duration": 3.288942,
     "end_time": "2024-04-07T07:37:56.995315",
     "exception": false,
     "start_time": "2024-04-07T07:37:53.706373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'spa_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 208.364807,
   "end_time": "2024-04-07T07:37:59.528049",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-07T07:34:31.163242",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
