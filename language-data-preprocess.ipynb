{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7948c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:05:49.058678Z",
     "iopub.status.busy": "2024-04-08T00:05:49.057924Z",
     "iopub.status.idle": "2024-04-08T00:06:06.938806Z",
     "shell.execute_reply": "2024-04-08T00:06:06.937546Z"
    },
    "papermill": {
     "duration": 17.894019,
     "end_time": "2024-04-08T00:06:06.941983",
     "exception": false,
     "start_time": "2024-04-08T00:05:49.047964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 00:05:51.701522: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 00:05:51.701654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 00:05:51.902192: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e81305",
   "metadata": {
    "papermill": {
     "duration": 0.007194,
     "end_time": "2024-04-08T00:06:06.956985",
     "exception": false,
     "start_time": "2024-04-08T00:06:06.949791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc36e0",
   "metadata": {
    "papermill": {
     "duration": 0.00702,
     "end_time": "2024-04-08T00:06:06.971603",
     "exception": false,
     "start_time": "2024-04-08T00:06:06.964583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2252e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:06.988458Z",
     "iopub.status.busy": "2024-04-08T00:06:06.987727Z",
     "iopub.status.idle": "2024-04-08T00:06:06.994943Z",
     "shell.execute_reply": "2024-04-08T00:06:06.994088Z"
    },
    "papermill": {
     "duration": 0.018211,
     "end_time": "2024-04-08T00:06:06.997141",
     "exception": false,
     "start_time": "2024-04-08T00:06:06.978930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t')[:2] for line in lines]\n",
    "\n",
    "    src = np.array([src for tgt, src in pairs])\n",
    "    tgt = np.array([tgt for tgt, src in pairs])\n",
    "\n",
    "    return tgt, src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c8d0a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:07.015251Z",
     "iopub.status.busy": "2024-04-08T00:06:07.013841Z",
     "iopub.status.idle": "2024-04-08T00:06:07.076072Z",
     "shell.execute_reply": "2024-04-08T00:06:07.074956Z"
    },
    "papermill": {
     "duration": 0.07414,
     "end_time": "2024-04-08T00:06:07.078853",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.004713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_txt = '/kaggle/input/translator-data/kor.txt'\n",
    "path = pathlib.Path(path_txt)\n",
    "tgt_raw, src_raw = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa0cf5",
   "metadata": {
    "papermill": {
     "duration": 0.006937,
     "end_time": "2024-04-08T00:06:07.093236",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.086299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Create a tf.data Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f545f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:07.109568Z",
     "iopub.status.busy": "2024-04-08T00:06:07.109104Z",
     "iopub.status.idle": "2024-04-08T00:06:07.420110Z",
     "shell.execute_reply": "2024-04-08T00:06:07.418648Z"
    },
    "papermill": {
     "duration": 0.323071,
     "end_time": "2024-04-08T00:06:07.423602",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.100531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(src_raw)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(tgt_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((src_raw[is_train], tgt_raw[is_train])))\n",
    "    #.shuffle(BUFFER_SIZE)\n",
    "    #.batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((src_raw[~is_train], tgt_raw[~is_train])))\n",
    "    #.shuffle(BUFFER_SIZE)\n",
    "    #.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14037a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:07.441424Z",
     "iopub.status.busy": "2024-04-08T00:06:07.440299Z",
     "iopub.status.idle": "2024-04-08T00:06:07.487481Z",
     "shell.execute_reply": "2024-04-08T00:06:07.486193Z"
    },
    "papermill": {
     "duration": 0.058854,
     "end_time": "2024-04-08T00:06:07.489934",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.431080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  ê°€.\n",
      "Target:  Go.\n"
     ]
    }
   ],
   "source": [
    "for src, tgt in train_raw.take(1):\n",
    "    print(\"Source: \", src.numpy().decode('utf-8'))\n",
    "    print(\"Target: \", tgt.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffdfa2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:07.507206Z",
     "iopub.status.busy": "2024-04-08T00:06:07.506124Z",
     "iopub.status.idle": "2024-04-08T00:06:07.558802Z",
     "shell.execute_reply": "2024-04-08T00:06:07.557678Z"
    },
    "papermill": {
     "duration": 0.064275,
     "end_time": "2024-04-08T00:06:07.561688",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.497413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tgt = train_raw.map(lambda src, tgt: tgt)\n",
    "train_src = train_raw.map(lambda src, tgt: src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd15da4",
   "metadata": {
    "papermill": {
     "duration": 0.007208,
     "end_time": "2024-04-08T00:06:07.576359",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.569151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Generate the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e934bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:07.594208Z",
     "iopub.status.busy": "2024-04-08T00:06:07.593561Z",
     "iopub.status.idle": "2024-04-08T00:06:07.606385Z",
     "shell.execute_reply": "2024-04-08T00:06:07.605329Z"
    },
    "papermill": {
     "duration": 0.02543,
     "end_time": "2024-04-08T00:06:07.609128",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.583698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7662ed5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:07.626034Z",
     "iopub.status.busy": "2024-04-08T00:06:07.625638Z",
     "iopub.status.idle": "2024-04-08T00:06:07.631779Z",
     "shell.execute_reply": "2024-04-08T00:06:07.630558Z"
    },
    "papermill": {
     "duration": 0.017321,
     "end_time": "2024-04-08T00:06:07.634171",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.616850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c73410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:07.650868Z",
     "iopub.status.busy": "2024-04-08T00:06:07.650472Z",
     "iopub.status.idle": "2024-04-08T00:06:43.704643Z",
     "shell.execute_reply": "2024-04-08T00:06:43.703416Z"
    },
    "papermill": {
     "duration": 36.072183,
     "end_time": "2024-04-08T00:06:43.713817",
     "exception": false,
     "start_time": "2024-04-08T00:06:07.641634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 s, sys: 84.6 ms, total: 36.1 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "src_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_src.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3ce4e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:43.730849Z",
     "iopub.status.busy": "2024-04-08T00:06:43.730453Z",
     "iopub.status.idle": "2024-04-08T00:06:52.451034Z",
     "shell.execute_reply": "2024-04-08T00:06:52.449949Z"
    },
    "papermill": {
     "duration": 8.732537,
     "end_time": "2024-04-08T00:06:52.453828",
     "exception": false,
     "start_time": "2024-04-08T00:06:43.721291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.76 s, sys: 30.1 ms, total: 8.79 s\n",
      "Wall time: 8.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tgt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_tgt.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be102c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:52.471311Z",
     "iopub.status.busy": "2024-04-08T00:06:52.470899Z",
     "iopub.status.idle": "2024-04-08T00:06:52.475645Z",
     "shell.execute_reply": "2024-04-08T00:06:52.474761Z"
    },
    "papermill": {
     "duration": 0.016543,
     "end_time": "2024-04-08T00:06:52.478178",
     "exception": false,
     "start_time": "2024-04-08T00:06:52.461635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "054842d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:52.495608Z",
     "iopub.status.busy": "2024-04-08T00:06:52.494538Z",
     "iopub.status.idle": "2024-04-08T00:06:52.501181Z",
     "shell.execute_reply": "2024-04-08T00:06:52.500157Z"
    },
    "papermill": {
     "duration": 0.017801,
     "end_time": "2024-04-08T00:06:52.503531",
     "exception": false,
     "start_time": "2024-04-08T00:06:52.485730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_vocab_file('kor_vocab.txt', src_vocab)\n",
    "write_vocab_file('en_vocab.txt', tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "981883ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:52.520988Z",
     "iopub.status.busy": "2024-04-08T00:06:52.519970Z",
     "iopub.status.idle": "2024-04-08T00:06:52.537903Z",
     "shell.execute_reply": "2024-04-08T00:06:52.536929Z"
    },
    "papermill": {
     "duration": 0.029592,
     "end_time": "2024-04-08T00:06:52.540688",
     "exception": false,
     "start_time": "2024-04-08T00:06:52.511096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count,1], START)\n",
    "    ends = tf.fill([count,1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb923e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:52.557699Z",
     "iopub.status.busy": "2024-04-08T00:06:52.557249Z",
     "iopub.status.idle": "2024-04-08T00:06:52.564849Z",
     "shell.execute_reply": "2024-04-08T00:06:52.563632Z"
    },
    "papermill": {
     "duration": 0.019069,
     "end_time": "2024-04-08T00:06:52.567363",
     "exception": false,
     "start_time": "2024-04-08T00:06:52.548294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b3d470e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:52.584869Z",
     "iopub.status.busy": "2024-04-08T00:06:52.584492Z",
     "iopub.status.idle": "2024-04-08T00:06:52.600693Z",
     "shell.execute_reply": "2024-04-08T00:06:52.599809Z"
    },
    "papermill": {
     "duration": 0.027504,
     "end_time": "2024-04-08T00:06:52.602730",
     "exception": false,
     "start_time": "2024-04-08T00:06:52.575226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        self.tokenizer = tf_text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:   \n",
    "\n",
    "        # Include a tokenize signature for a batch of strings. \n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "              tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "              tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2,-1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54510119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:52.620103Z",
     "iopub.status.busy": "2024-04-08T00:06:52.619236Z",
     "iopub.status.idle": "2024-04-08T00:06:56.373556Z",
     "shell.execute_reply": "2024-04-08T00:06:56.372087Z"
    },
    "papermill": {
     "duration": 3.766027,
     "end_time": "2024-04-08T00:06:56.376422",
     "exception": false,
     "start_time": "2024-04-08T00:06:52.610395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.kor = CustomTokenizer(reserved_tokens, '/kaggle/working/kor_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, '/kaggle/working/en_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2067a09e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T00:06:56.393837Z",
     "iopub.status.busy": "2024-04-08T00:06:56.393454Z",
     "iopub.status.idle": "2024-04-08T00:06:59.516455Z",
     "shell.execute_reply": "2024-04-08T00:06:59.515334Z"
    },
    "papermill": {
     "duration": 3.13564,
     "end_time": "2024-04-08T00:06:59.519987",
     "exception": false,
     "start_time": "2024-04-08T00:06:56.384347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'kor_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4748462,
     "sourceId": 8051857,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 76.234657,
   "end_time": "2024-04-08T00:07:02.106358",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T00:05:45.871701",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
